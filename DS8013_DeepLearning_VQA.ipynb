{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhazary/git-vqa-model/blob/main/DS8013_DeepLearning_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqJZQIkb82rA"
      },
      "source": [
        "**Step 10: Training Loop**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4aDvZJp9Hf3"
      },
      "source": [
        "**Step 11: Saving the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra_Yb6Pn9pk8"
      },
      "source": [
        "**Step 12: Make Predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn2sN6Dx7ic-"
      },
      "source": [
        "**Step 1: Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRvd78WfAsKt",
        "outputId": "07250fca-0127-41ca-e6ce-d3a4f2b02ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Python Code to Load and Print Dataset Information in Google Colab\n",
        "\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')  # Mount Google Drive to access dataset\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/vqa_abstract_dataset\"  # Adjust this based on your Drive structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ_aGvy_7pW3"
      },
      "source": [
        "**Step 2: Define Dataset Paths and Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo382JMeCUOl"
      },
      "outputs": [],
      "source": [
        "# File paths\n",
        "mcq_train_file = os.path.join(dataset_dir, \"vqa_v1_abstract_mcq_questions_train2015.json\")\n",
        "mcq_val_file = os.path.join(dataset_dir, \"vqa_v1_abstract_mcq_questions_val2015.json\")\n",
        "annotations_train_file = os.path.join(dataset_dir, \"vqa_v1_abstract_annotations_train2015.json\")\n",
        "annotations_val_file = os.path.join(dataset_dir, \"vqa_v1_abstract_annotations_val2015.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AnA407ycTBC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the dataset directory\n",
        "base_dir = \"/content/drive/MyDrive/vqa_abstract_dataset\"\n",
        "\n",
        "# File paths\n",
        "mcq_train_file = os.path.join(base_dir, \"vqa_v1_abstract_mcq_questions_train2015.json.json\")\n",
        "mcq_val_file = os.path.join(base_dir, \"vqa_v1_abstract_mcq_questions_val2015.json.json\")\n",
        "annotations_train_file = os.path.join(base_dir, \"vqa_v1_abstract_annotations_train2015.json.json\")\n",
        "annotations_val_file = os.path.join(base_dir, \"vqa_v1_abstract_annotations_val2015.json.json\")\n",
        "\n",
        "# Load multiple-choice questions\n",
        "with open(mcq_train_file, \"r\") as f:\n",
        "    mcq_train = json.load(f)\n",
        "\n",
        "with open(mcq_val_file, \"r\") as f:\n",
        "    mcq_val = json.load(f)\n",
        "\n",
        "# Load annotations (answers)\n",
        "with open(annotations_train_file, \"r\") as f:\n",
        "    annotations_train = json.load(f)\n",
        "\n",
        "with open(annotations_val_file, \"r\") as f:\n",
        "    annotations_val = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQODpRfncvHi",
        "outputId": "a2e0922a-73b8-4ccb-c862-84a6f243d343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in multiple-choice questions: dict_keys(['info', 'task_type', 'data_type', 'license', 'questions', 'data_subtype', 'num_choices'])\n",
            "\n",
            "ðŸ”¹ Sample Multiple-Choice Question:\n",
            "{\n",
            "    \"image_id\": 11779,\n",
            "    \"question\": \"Who looks happier?\",\n",
            "    \"multiple_choices\": [\n",
            "        \"alive\",\n",
            "        \"1\",\n",
            "        \"woman\",\n",
            "        \"purple\",\n",
            "        \"2\",\n",
            "        \"yes\",\n",
            "        \"white\",\n",
            "        \"boy\",\n",
            "        \"she loves him\",\n",
            "        \"mountain\",\n",
            "        \"3\",\n",
            "        \"no\",\n",
            "        \"baby\",\n",
            "        \"man\",\n",
            "        \"yellow\",\n",
            "        \"red\",\n",
            "        \"4\",\n",
            "        \"blue\"\n",
            "    ],\n",
            "    \"question_id\": 117792\n",
            "}\n",
            "\n",
            "ðŸ”¹ Sample Answer:\n",
            "{\n",
            "    \"question_type\": \"who\",\n",
            "    \"multiple_choice_answer\": \"man\",\n",
            "    \"answers\": [\n",
            "        {\n",
            "            \"answer\": \"old person\",\n",
            "            \"answer_confidence\": \"maybe\",\n",
            "            \"answer_id\": 1\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"maybe\",\n",
            "            \"answer_id\": 2\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 3\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 4\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"old man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 5\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 6\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 7\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 8\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 9\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"grandpa\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 10\n",
            "        }\n",
            "    ],\n",
            "    \"image_id\": 11779,\n",
            "    \"answer_type\": \"other\",\n",
            "    \"question_id\": 117792\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Print dataset structure\n",
        "print(\"Keys in multiple-choice questions:\", mcq_train.keys())\n",
        "print(\"\\nðŸ”¹ Sample Multiple-Choice Question:\")\n",
        "print(json.dumps(mcq_train[\"questions\"][0], indent=4))\n",
        "\n",
        "print(\"\\nðŸ”¹ Sample Answer:\")\n",
        "print(json.dumps(annotations_train[\"annotations\"][0], indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKIVr_hb7xHU"
      },
      "source": [
        "**Step 3: Image Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKW9Ntggcxwe"
      },
      "outputs": [],
      "source": [
        "# Image directory\n",
        "image_dir = os.path.join(dataset_dir, \"scene_img_abstract_v002_train2015\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "CbiVTs-Nczxs",
        "outputId": "2f2e2869-aaa5-4e70-b5dc-922eb7e09b6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ðŸ”¹ Step 2: Display an Image with a Question & Answer\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Path to image directory\n",
        "image_dir = os.path.join(dataset_dir, \"scene_img_abstract_v002_train2015\")\n",
        "\n",
        "# Pick a sample multiple-choice question\n",
        "sample_mc_question = mcq_train[\"questions\"][0]  # First question\n",
        "sample_image_id = sample_mc_question[\"image_id\"]\n",
        "\n",
        "# Find the corresponding answer\n",
        "sample_annotation = next(\n",
        "    ann for ann in annotations_train[\"annotations\"] if ann[\"image_id\"] == sample_image_id\n",
        ")\n",
        "\n",
        "# Load and display image\n",
        "sample_image_path = os.path.join(image_dir, f\"abstract_v002_train2015_{sample_image_id:012d}.png\")\n",
        "\n",
        "\n",
        "if os.path.exists(sample_image_path):\n",
        "    img = Image.open(sample_image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Q: {sample_mc_question['question']}\\nA: {sample_annotation['multiple_choice_answer']}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Image not found:\", sample_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_nll1BbMCyP",
        "outputId": "66c238a8-0fb6-4e8b-881e-db415643d127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 159MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Image Shape: torch.Size([3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "#  Step 1: Preprocessing Images\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ðŸ”¹ Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ”¹ Define ResNet-50 model and remove final classification layer\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
        "resnet.to(device)  # Move ResNet-50 to GPU if available\n",
        "\n",
        "# Define image transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load an image, apply transformations, and return a tensor.\"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
        "    img_tensor = transform(img)\n",
        "    return img_tensor\n",
        "\n",
        "# ðŸ”¹ Feature extraction function using ResNet-50\n",
        "def extract_image_features(image_tensor):\n",
        "    \"\"\"Extract features from the image using ResNet-50.\"\"\"\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "    with torch.no_grad():  # No need for gradients during feature extraction\n",
        "        features = resnet(image_tensor)  # Features shape: [batch_size, 2048, 1, 1]\n",
        "        features = features.view(features.size(0), -1)  # Flatten to [batch_size, 2048]\n",
        "    return features\n",
        "\n",
        "# Test with a sample image\n",
        "sample_image_tensor = preprocess_image(sample_image_path)\n",
        "print(\"Processed Image Shape:\", sample_image_tensor.shape)  # Expected: [3, 224, 224]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQiuIwT7fm7W",
        "outputId": "3515be3b-eb17-47b8-a3a3-3889d1257aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013227.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013226.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013223.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013228.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013225.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013224.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013232.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013229.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013230.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013244.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013239.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013235.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013238.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013242.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013243.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013236.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013237.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013234.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013240.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013241.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013233.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013253.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013246.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013252.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013249.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013248.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013250.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013247.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013251.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013245.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013257.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013261.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013262.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013255.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013256.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013254.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013258.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013259.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013260.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013265.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013267.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013268.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013263.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013269.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013266.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013264.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013274.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013273.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013272.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013277.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013276.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013270.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013275.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013271.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013282.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013279.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013281.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013280.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013283.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013278.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013288.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013290.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013289.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013285.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013291.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013286.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013284.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013287.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013295.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013296.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013293.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013297.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013292.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013294.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013302.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013304.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013305.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013301.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013303.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013298.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013300.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013299.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013308.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013309.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013311.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013306.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013310.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013307.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013312.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013315.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013318.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013319.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013316.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013314.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013317.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013313.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013321.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013323.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013320.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013324.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013322.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013326.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013328.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013329.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013327.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013325.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013332.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013331.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013330.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013333.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013335.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013334.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013336.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013339.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013337.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013338.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013341.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013342.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013343.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013340.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013348.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013344.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013346.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013347.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013349.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013345.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013350.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013352.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013353.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013351.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013360.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013357.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013354.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013358.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013356.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013361.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013355.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013359.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013367.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013362.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013364.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013366.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013368.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013365.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013363.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013373.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013372.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013375.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013370.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013376.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013369.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013371.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013374.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013381.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013384.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013378.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013382.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013383.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013379.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013385.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013377.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013380.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013390.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013392.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013386.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013387.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013395.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013393.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013391.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013389.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013394.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013388.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013407.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013401.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013402.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013406.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013403.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013404.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013400.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013397.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013399.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013398.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013405.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013396.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013417.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013413.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013412.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013414.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013416.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013409.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013410.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013408.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013411.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013418.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013415.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013419.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013424.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013422.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013429.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013430.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013427.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013420.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013426.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013421.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013428.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013423.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013425.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013438.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013437.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013432.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013433.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013435.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013436.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013439.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013434.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013441.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013440.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013431.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013447.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013444.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013442.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013451.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013454.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013453.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013448.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013446.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013449.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013443.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013450.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013445.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013452.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013461.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013460.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013457.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013456.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013463.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013459.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013466.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013464.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013455.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013465.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013458.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013462.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013469.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013468.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013476.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013472.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013474.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013467.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013477.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013473.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013475.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013471.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013470.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013480.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013483.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013481.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013479.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013486.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013482.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013485.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013478.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013484.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013487.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013488.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013490.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013493.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013494.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013492.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013491.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013489.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013498.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013496.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013497.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013499.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013495.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013500.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013505.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013502.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013504.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013506.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013503.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013501.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013510.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013513.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013514.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013507.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013509.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013508.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013511.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013512.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013515.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013518.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013516.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013517.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013520.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013521.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013522.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013519.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013524.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013525.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013523.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013526.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013528.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013527.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013530.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013529.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013534.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013531.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013532.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013533.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013538.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013537.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013539.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013536.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013540.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013541.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013535.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013547.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013548.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013542.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013544.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013546.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013543.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013545.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013552.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013554.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013553.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013555.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013549.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013550.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013556.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013551.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013566.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013563.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013564.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013558.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013567.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013565.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013562.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013557.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013560.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013559.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013561.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013578.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013579.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013576.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013571.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013570.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013574.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013569.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013575.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013572.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013568.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013577.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013573.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013590.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013586.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013583.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013588.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013587.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013580.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013584.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013581.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013582.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013589.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013585.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013594.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013602.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013601.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013597.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013595.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013591.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013600.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013598.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013599.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013596.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013593.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013592.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013611.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013615.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013605.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013609.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013607.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013608.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013610.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013603.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013606.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013612.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013613.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013614.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013604.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013627.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013616.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013617.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013623.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013626.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013624.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013618.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013620.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013619.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013625.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013622.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013621.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013633.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013631.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013639.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013630.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013632.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013640.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013634.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013636.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013628.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013629.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013637.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013635.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013638.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013643.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013648.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013650.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013649.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013645.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013651.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013644.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013646.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013641.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013647.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013642.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013652.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013653.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013663.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013660.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013656.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013654.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013661.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013658.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013659.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013657.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013662.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013664.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013655.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013665.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013672.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013674.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013671.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013667.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013673.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013670.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013669.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013675.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013666.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013676.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013677.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013668.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013686.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013679.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013687.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013690.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013685.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013678.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013682.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013680.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013688.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013681.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013689.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013684.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013683.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013695.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013691.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013700.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013694.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013692.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013698.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013699.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013693.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013701.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013696.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013697.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013708.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013711.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013706.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013712.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013702.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013709.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013710.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013713.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013714.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013705.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013715.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013704.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013703.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013707.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013721.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013720.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013718.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013717.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013723.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013726.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013722.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013724.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013716.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013725.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013719.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013730.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013734.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013732.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013727.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013731.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013728.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013735.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013737.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013729.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013736.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013733.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013748.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013742.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013739.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013744.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013741.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013749.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013745.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013746.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013738.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013740.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013743.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013747.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013750.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013751.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013762.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013753.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013755.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013758.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013754.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013761.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013756.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013757.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013759.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013752.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013760.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013777.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013763.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013767.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013770.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013772.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013776.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013769.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013774.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013768.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013764.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013775.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013765.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013771.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013773.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013766.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013788.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013778.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013781.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013789.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013785.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013779.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013782.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013784.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013790.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013786.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013783.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013780.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013787.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013799.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013797.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013793.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013792.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013796.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013791.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013795.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013794.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013798.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013810.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013804.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013811.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013802.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013805.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000013800.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008093 (3).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008092 (3).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008093 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008093 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008092 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008092 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008092.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008093.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008105 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008104 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008109 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008107 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008106 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008106.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008105 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008108 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008105.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008109 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008107 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008108.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008107.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008109.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008106 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008108 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008104 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000008104.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011832.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011839.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011840.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011837.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011833.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011836.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011834.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011841.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011838.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011835.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011847.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011857.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011842.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011845.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011854.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011843.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011853.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011856.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011859.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011852.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011849.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011846.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011855.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011850.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011844.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011858.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011851.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011848.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011872.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011866.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011865.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011861.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011867.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011869.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011868.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011873.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011871.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011877.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011878.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011879.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011870.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011864.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011862.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011875.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011874.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011876.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011863.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011860.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011889.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011896.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011894.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011891.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011887.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011897.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011895.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011880.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011885.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011892.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011884.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011883.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011886.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011888.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011881.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011893.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011882.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011890.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011916.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011911.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011901.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011913.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011914.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011902.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011907.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011910.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011915.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011909.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011904.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011900.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011898.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011903.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011908.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011905.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011899.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011906.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011912.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011919.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011924.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011921.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011930.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011936.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011918.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011920.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011923.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011935.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011929.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011931.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011932.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011928.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011925.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011934.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011927.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011922.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011917.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011933.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011926.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011953.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011951.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011955.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011947.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011948.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011956.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011944.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011946.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011941.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011952.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011945.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011943.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011937.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011939.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011949.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011950.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011940.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011942.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011938.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011954.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011966.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011968.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011960.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011959.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011961.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011969.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011957.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011973.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011972.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011965.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011967.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011970.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011974.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011964.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011962.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011958.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011971.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011963.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011975.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011976.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011990.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011980.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011988.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011978.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011983.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011989.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011977.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011987.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011984.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011981.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011986.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011979.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011982.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011985.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012003.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011998.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012007.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012002.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011999.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011997.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011991.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011993.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011996.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012001.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012000.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011994.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011992.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012005.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012006.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012004.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000011995.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012010.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012014.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012017.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012013.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012012.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012008.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012015.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012011.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012016.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012024.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012022.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012019.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012023.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012018.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012020.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012009.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012021.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012028.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012040.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012031.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012026.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012027.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012037.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012030.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012025.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012034.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012033.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012032.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012039.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012036.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012038.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012035.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012029.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012051.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012041.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012050.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012045.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012049.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012047.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012055.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012044.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012054.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012043.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012048.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012046.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012053.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012052.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012042.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012056.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012059.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012058.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012067.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012064.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012061.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012063.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012060.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012066.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012057.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012062.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012065.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012074.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012076.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012081.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012072.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012075.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012079.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012071.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012080.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012069.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012082.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012070.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012073.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012077.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012068.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012083.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012078.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012088.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012090.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012098.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012099.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012096.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012097.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012091.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012095.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012100.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012092.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012084.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012087.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012094.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012093.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012086.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012089.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012085.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012101.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012106.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012109.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012103.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012107.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012104.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012108.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012110.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012105.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012102.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012119.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012118.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012120.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012111.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012113.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012114.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012116.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012112.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012115.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012121.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012117.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012122.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012123.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012129.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012126.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012125.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012130.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012127.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012124.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012128.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012140.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012137.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012132.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012134.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012133.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012142.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012131.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012139.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012141.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012138.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012135.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012136.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012144.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012153.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012147.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012146.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012149.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012151.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012152.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012148.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012145.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012143.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012150.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012154.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012159.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012156.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012155.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012157.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012162.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012164.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012163.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012161.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012160.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012158.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012166.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012170.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012171.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012169.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012167.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012165.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012168.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012178.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012177.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012174.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012175.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012179.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012180.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012176.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012173.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012172.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012186.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012182.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012189.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012192.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012191.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012184.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012194.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012187.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012185.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012181.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012193.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012188.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012190.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012183.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012203.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012205.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012195.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012201.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012196.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012197.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012207.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012208.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012204.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012206.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012202.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012200.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012198.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012199.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012224.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012219.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012217.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012216.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012220.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012214.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012213.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012211.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012218.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012210.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012223.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012215.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012222.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012212.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012221.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012225.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012209.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012239.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012234.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012231.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012233.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012237.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012230.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012235.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012232.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012236.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012228.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012238.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012226.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012242.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012240.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012227.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012241.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012229.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012260.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012250.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012248.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012252.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012249.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012262.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012255.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012259.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012251.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012246.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012243.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012256.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012244.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012254.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012253.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012245.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012258.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012257.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012261.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012247.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012266.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012268.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012278.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012264.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012263.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012270.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012277.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012276.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012279.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012265.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012272.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012269.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012274.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012271.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012267.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012273.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012275.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012291.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012286.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012295.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012280.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012290.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012283.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012294.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012284.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012288.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012287.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012289.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012285.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012282.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012292.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012281.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012293.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012302.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012296.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012310.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012300.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012299.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012308.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012305.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012301.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012303.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012306.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012298.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012307.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012304.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012297.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012309.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012314.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012327.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012320.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012316.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012326.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012323.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012322.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012321.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012324.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012325.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012312.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012319.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012311.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012313.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012315.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012318.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012317.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012342.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012328.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012334.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012339.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012344.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012332.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012333.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012335.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012341.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012331.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012329.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012343.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012336.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012340.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012330.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012337.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012338.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012347.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012352.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012353.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012345.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012357.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012348.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012356.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012351.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012350.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012355.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012346.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012349.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012358.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012354.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012359.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012363.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012374.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012366.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012364.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012371.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012362.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012369.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012361.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012368.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012365.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012372.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012367.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012360.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012373.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012370.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012386.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012389.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012384.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012391.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012388.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012380.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012375.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012390.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012387.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012385.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012376.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012381.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012383.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012378.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012392.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012377.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012379.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012382.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012407.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012401.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012398.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012408.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012397.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012399.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012393.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012402.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012400.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012394.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012406.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012396.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012405.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012409.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012404.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012395.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012403.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012410.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012417.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012416.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012413.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012412.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012414.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012425.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012418.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012419.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012422.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012423.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012420.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012415.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012424.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012411.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012421.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012432.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012426.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012429.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012433.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012430.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012428.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012427.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012431.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012441.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012434.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012436.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012440.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012435.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012442.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012439.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012437.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012438.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012444.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012449.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012450.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012451.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012445.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012448.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012447.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012446.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012452.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012443.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012453.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012455.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012459.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012454.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012458.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012460.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012457.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012456.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012463.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012467.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012470.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012461.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012468.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012465.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012466.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012472.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012471.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012464.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012462.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012469.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012474.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012481.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012479.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012478.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012483.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012482.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012475.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012476.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012473.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012477.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012480.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012489.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012494.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012495.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012488.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012484.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012490.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012485.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012491.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012492.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012486.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012493.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012487.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012504.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012497.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012502.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012507.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012499.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012501.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012503.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012506.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012498.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012505.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012496.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012500.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012508.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012515.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012514.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012510.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012517.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012516.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012519.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012509.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012511.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012512.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012513.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012518.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012530.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012529.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012528.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012527.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012523.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012526.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012522.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012521.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012520.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012525.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012524.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012531.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012542.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012535.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012536.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012534.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012538.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012539.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012537.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012532.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012541.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012533.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012540.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012543.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012544.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012546.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012548.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012554.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012547.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012550.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012545.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012549.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012553.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012552.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012551.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012555.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012557.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012559.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012561.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012560.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012566.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012556.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012562.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012564.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012558.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012567.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012563.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012565.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012578.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012570.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012571.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012577.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012569.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012576.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012573.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012579.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012572.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012568.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012575.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012574.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012580.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012582.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012584.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012586.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012583.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012588.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012591.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012589.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012590.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012585.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012587.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012581.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012600.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012595.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012603.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012605.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012602.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012604.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012592.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012593.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012601.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012596.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012594.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012597.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012598.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012599.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012617.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012616.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012613.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012612.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012609.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012606.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012611.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012608.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012610.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012618.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012607.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012614.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012615.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012631.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012629.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012632.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012622.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012625.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012633.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012628.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012624.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012626.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012620.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012619.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012630.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012623.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012621.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012627.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012635.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012637.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012634.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012643.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012642.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012648.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012641.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012638.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012639.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012644.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012645.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012647.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012646.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012636.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012640.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012649.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012660.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012661.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012655.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012657.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012662.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012656.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012654.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012658.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012653.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012659.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012650.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012652.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012651.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012670.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012666.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012676.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012672.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012669.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012668.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012663.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012667.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012675.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012665.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012674.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012673.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012671.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012664.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012683.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012679.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012677.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012678.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012681.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012688.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012687.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012680.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012690.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012685.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012686.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012682.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012689.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012684.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012692.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012693.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012698.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012699.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012696.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012697.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012694.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012691.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012695.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012701.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012703.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012700.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012704.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012702.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012717.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012705.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012710.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012707.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012714.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012711.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012712.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012709.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012716.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012706.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012715.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012713.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012708.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012730.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012732.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012725.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012721.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012720.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012726.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012728.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012718.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012727.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012724.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012719.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012731.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012722.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012729.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012723.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012733.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012738.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012736.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012737.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012739.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012735.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012734.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012743.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012740.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012741.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012744.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012742.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012750.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012747.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012752.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012751.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012745.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012748.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012746.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012749.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012756.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012755.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012757.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012754.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012761.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012760.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012759.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012753.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012758.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012766.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012771.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012770.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012762.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012764.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012767.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012772.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012768.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012765.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012773.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012763.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012769.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012777.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012781.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012774.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012779.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012776.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012780.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012778.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012775.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012787.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012784.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012786.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012785.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012783.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012782.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012790.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012789.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012791.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012788.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012797.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012798.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012795.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012793.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012796.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012794.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012799.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012800.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012792.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012806.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012805.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012801.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012808.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000012803.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001559 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001562 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001556 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001562 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001560 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001567 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001563 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001560 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001561 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001566 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001566 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001559.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001566.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001567 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001558 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001563 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001558 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001561 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001562.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001556 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001557 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001561.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001557 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001563.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001557.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001560.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001567.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001559 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001558.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000001556.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007906 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007908 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007912 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007907 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007911 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007907.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007906.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007909 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007910 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007906 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007911 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007912 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007909.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007913 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007910 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007910.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007911.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007908 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007913.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007912.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007908.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007907 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007913 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007909 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007970 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007969 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007964 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007970.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007971 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007969 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007971 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007964 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007966 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007964.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007965 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007966.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007967 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007974 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007967 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007966 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007968 (1) (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007969.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007965 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007968 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007974 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007967.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007975 (2).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007968.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007975.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007965.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007970 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007974.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007975 (1).png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Processing image: /content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000007971.png\n",
            "Preprocessed tensor shape: torch.Size([3, 224, 224])\n",
            "Extracted features shape: torch.Size([2048])\n",
            "Extracted Image Features Shape: torch.Size([20084, 2048])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Assuming preprocess_image and extract_image_features are defined elsewhere\n",
        "# Example function placeholders:\n",
        "def preprocess_image(image_path):\n",
        "    # Replace with actual image preprocessing logic\n",
        "    return torch.rand((3, 224, 224))  # Example tensor simulating an image tensor\n",
        "\n",
        "def extract_image_features(image_tensor):\n",
        "    # Replace with actual feature extraction logic\n",
        "    return torch.rand((2048,))  # Example tensor simulating feature extraction\n",
        "\n",
        "# Directory containing images\n",
        "image_dir = os.path.join(dataset_dir, \"scene_img_abstract_v002_train2015\")\n",
        "\n",
        "# Initialize lists to store features and image IDs\n",
        "image_features_list = []\n",
        "image_ids = []\n",
        "\n",
        "# Process images in the directory\n",
        "for image_name in os.listdir(image_dir):\n",
        "    # Check for valid image file extensions\n",
        "    if image_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "        image_path = os.path.join(image_dir, image_name)\n",
        "        print(f\"Processing image: {image_path}\")\n",
        "        try:\n",
        "            # Preprocess the image\n",
        "            image_tensor = preprocess_image(image_path)\n",
        "            print(f\"Preprocessed tensor shape: {image_tensor.shape}\")\n",
        "\n",
        "            # Extract features\n",
        "            image_features = extract_image_features(image_tensor)\n",
        "            print(f\"Extracted features shape: {image_features.shape}\")\n",
        "\n",
        "            # Store features and image ID\n",
        "            image_features_list.append(image_features)\n",
        "            image_ids.append(image_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_name}: {e}\")\n",
        "\n",
        "# Verify and stack extracted features into a tensor\n",
        "if image_features_list:\n",
        "    image_features_tensor = torch.stack(image_features_list)\n",
        "    print(f\"Extracted Image Features Shape: {image_features_tensor.shape}\")  # Should be [num_images, 2048]\n",
        "else:\n",
        "    print(\"No features were extracted. Please check preprocessing and feature extraction logic.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHLYQ7OOfBb-",
        "outputId": "983e3683-e793-41b4-a118-37eeb92417ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20084 files in the directory.\n",
            "No image features extracted.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ðŸ”¹ Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ”¹ Define ResNet-50 model and remove final classification layer\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
        "resnet.to(device)  # Move ResNet-50 to GPU if available\n",
        "\n",
        "# ðŸ”¹ Define image transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ResNet\n",
        "])\n",
        "\n",
        "# ðŸ”¹ Preprocessing function for image\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load an image, apply transformations, and return a tensor.\"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
        "    img_tensor = transform(img)\n",
        "    return img_tensor\n",
        "\n",
        "# ðŸ”¹ Feature extraction function using ResNet-50\n",
        "def extract_image_features(image_tensor):\n",
        "    \"\"\"Extract features from the image using ResNet-50.\"\"\"\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "    with torch.no_grad():  # No need for gradients during feature extraction\n",
        "        features = resnet(image_tensor)  # Features shape: [batch_size, 2048, 1, 1]\n",
        "        features = features.view(features.size(0), -1)  # Flatten to [batch_size, 2048]\n",
        "    return features\n",
        "\n",
        "# ðŸ”¹ Set dataset directory (adjust this as needed)\n",
        "dataset_dir = \"/content/drive/MyDrive/vqa_abstract_dataset\"  # Adjust to your dataset directory\n",
        "image_dir = os.path.join(dataset_dir, \"scene_img_abstract_v002_train2015\")\n",
        "\n",
        "# ðŸ”¹ Extract features for all images in the directory\n",
        "image_features_list = []\n",
        "image_ids = []  # Store image IDs if needed for later use\n",
        "\n",
        "# Check if the directory exists and contains files\n",
        "if not os.path.exists(image_dir):\n",
        "    print(f\"Directory {image_dir} does not exist.\")\n",
        "else:\n",
        "    image_files = os.listdir(image_dir)\n",
        "    print(f\"Found {len(image_files)} files in the directory.\")\n",
        "\n",
        "    for image_name in image_files:\n",
        "        if image_name.endswith(\".jpg\"):  # Assuming the images are in .jpg format\n",
        "            image_path = os.path.join(image_dir, image_name)\n",
        "\n",
        "            try:\n",
        "                # Preprocess and extract features\n",
        "                image_tensor = preprocess_image(image_path)\n",
        "                image_features = extract_image_features(image_tensor)\n",
        "\n",
        "                # Store features and image ID\n",
        "                image_features_list.append(image_features)\n",
        "                image_ids.append(image_name)  # Or extract image ID from annotations if available\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_name}: {e}\")\n",
        "\n",
        "# Check if image_features_list is not empty before stacking\n",
        "if image_features_list:\n",
        "    image_features_tensor = torch.stack(image_features_list)\n",
        "    print(f\"Extracted Image Features Shape: {image_features_tensor.shape}\")  # Should be [num_images, 2048]\n",
        "else:\n",
        "    print(\"No image features extracted.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daA2o4MU72Z1"
      },
      "source": [
        "**Step 4: Text Tokenization Using BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "4cb550a02504421b860db7b4a046edea",
            "f42b7bb539924cbd9f3c4c5641a3a248",
            "8b5c97fcef614be88baff2164720d0b9",
            "5f8e9872452446a0ab4870f2edb5c543",
            "c6a5c81450444f7ca98d83ab162156d8",
            "801a41e7449c4604b96946dffee9d3ca",
            "e7a32c777a414dbe94b3eb65a4209894",
            "5496531b69504e348936569cfbfc4c52",
            "0e572cdfcccc4229836add10e8544e31",
            "e796e9652b2e4b3cab3d8dece7e22bee",
            "de3831d00cfc40eb910b35bed4daf86d",
            "c3da1cb626a24418a4c94583709b1304",
            "6a153faae67e4df2b2db22ef77dd0557",
            "d3a8f94c0ed746e49eac52d825985205",
            "4dcac941ba314319b9ab320ddcbcbf35",
            "e3e1ff04016144308f8ce4e973c00b77",
            "a126fd834f2942bfb7939220ba71d55b",
            "2af0a2cc51984cbe938fcd6d74be05a2",
            "8ccfcbb680d04dd39c166b756d9192e4",
            "74c39715aaaf4d85b510e4a285c810fb",
            "4412530bcac945738847453724c1cf8f",
            "ebfb2e30104e4f4b931cc75f5564f51e",
            "ab0de5bcf3c048538e79957abc93765c",
            "8d39c33840c94952b6a40ed8c93dbb9e",
            "ef6a51e1ae244b4aad333f3f3be6a640",
            "970366c634db4ded990ce5c700dbc6fe",
            "fdca40e819ef4c22b9901878e6a2959c",
            "d9a110bfb4e546b9afd587794965461f",
            "5ef1fad53c134cbd9b057890ef060500",
            "2bd6695a935548709c0ed6ab10344e6e",
            "8bb9cc6c4c094ce18d0ebc26ff596bed",
            "dae82c4407ed4e45a254c51d58c5b6d8",
            "e92565ce26814c2d903bbe51f42c4faf",
            "f807ddaa4959446c954e84bc91aff463",
            "175bcdb734664fe19546bf68090f2606",
            "456c2303139f4654a32cb3d85cd377f8",
            "915a9890db86458792c57166bfb7be4f",
            "08a257b8c4bf41ee90b14a1eb325dd21",
            "7ec25e27bc3047dab11941948424558e",
            "9a1d07bb852549509beebdbffa12cb40",
            "a1ed8c70bd60488688a214df0b86cb62",
            "e5289ac724324107ad10953adb44d572",
            "15b0e36810944907b932ddf1cfeb14e5",
            "87d3da753db745b686205112d97dd63d"
          ]
        },
        "id": "JS3dncExMKVQ",
        "outputId": "f748c975-a444-406d-e275-acfe0ce9d08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cb550a02504421b860db7b4a046edea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3da1cb626a24418a4c94583709b1304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab0de5bcf3c048538e79957abc93765c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f807ddaa4959446c954e84bc91aff463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Question: tensor([ 101, 2054, 2003, 1996, 2711, 3173, 1029,  102,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# ðŸ”¹ Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ”¹ Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_question(question, max_length=30):\n",
        "    \"\"\"Tokenizes a question and returns input tensors on GPU.\"\"\"\n",
        "    encoded = tokenizer(\n",
        "        question,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,  # Dynamic max_length\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = encoded[\"input_ids\"].squeeze(0).to(device)  # Shape: [seq_len]\n",
        "    attention_mask = encoded[\"attention_mask\"].squeeze(0).to(device)  # Shape: [seq_len]\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "# ðŸ”¹ Test with Sample Data (Question)\n",
        "sample_question = \"What is the person holding?\"\n",
        "input_ids, attention_mask = preprocess_question(sample_question)\n",
        "\n",
        "print(\"Tokenized Question:\", input_ids)\n",
        "print(\"Attention Mask:\", attention_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp7s_HAL76nD"
      },
      "source": [
        "**Step 5: Answer Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MbTO0nsMRUo",
        "outputId": "b3cb70d3-009a-4952-9ee7-f1ae88e2d655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: man\n",
            "Encoded Answer Label: 1530\n"
          ]
        }
      ],
      "source": [
        "#ðŸ”¹ Step 3: Preprocessing Answers\n",
        "\n",
        "# Python Code for Answer Encoding\n",
        "\n",
        "# Collect all unique answers to create a vocabulary\n",
        "all_answers = [ann[\"multiple_choice_answer\"] for ann in annotations_train[\"annotations\"]]\n",
        "answer_vocab = {ans: idx for idx, ans in enumerate(set(all_answers))}  # Create dictionary\n",
        "\n",
        "def preprocess_answer(answer):\n",
        "    \"\"\"Converts an answer into a numerical label.\"\"\"\n",
        "    return answer_vocab.get(answer, -1)  # Return -1 if the answer is not found in vocab\n",
        "\n",
        "# Test with a sample answer\n",
        "sample_answer = annotations_train[\"annotations\"][0][\"multiple_choice_answer\"]\n",
        "answer_label = preprocess_answer(sample_answer)\n",
        "\n",
        "print(\"Answer:\", sample_answer)\n",
        "print(\"Encoded Answer Label:\", answer_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbfAZUr979xJ"
      },
      "source": [
        "**Step 6: Dataset Creation (VQADataset)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Vd-el1MaoQ"
      },
      "outputs": [],
      "source": [
        "# ðŸ”¹ Step 4: Creating the Final Dataset\n",
        "# Python Code to Create the Final Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, questions, annotations, image_dir):\n",
        "        self.questions = questions[\"questions\"]\n",
        "        self.annotations = annotations[\"annotations\"]\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get question and answer\n",
        "        question_text = self.questions[idx][\"question\"]\n",
        "        answer_text = self.annotations[idx][\"multiple_choice_answer\"]\n",
        "\n",
        "        # Get image ID and load image\n",
        "        image_id = self.questions[idx][\"image_id\"]\n",
        "        image_path = os.path.join(self.image_dir, f\"abstract_v002_train2015_{image_id:012d}.png\")\n",
        "\n",
        "        # Preprocess image, question, and answer\n",
        "        image_tensor = preprocess_image(image_path)\n",
        "        input_ids, attention_mask = preprocess_question(question_text)\n",
        "        answer_label = preprocess_answer(answer_text)\n",
        "\n",
        "        return image_tensor, input_ids, attention_mask, answer_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RvV51LG8Nwn"
      },
      "source": [
        "**Step 7: DataLoader Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAL9egddXfZd",
        "outputId": "3c1005fd-9c27-48f1-c601-ad101668b0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Image Shape: torch.Size([16, 3, 224, 224])\n",
            "Batch Question Shape: torch.Size([16, 30])\n",
            "Batch Answer Shape: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# Initialize dataset\n",
        "vqa_dataset = VQADataset(mcq_train, annotations_train, image_dir)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(vqa_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Test a batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"Batch Image Shape:\", batch[0].shape)  # Expected: [batch_size, 3, 224, 224]\n",
        "print(\"Batch Question Shape:\", batch[1].shape)  # Expected: [batch_size, 20]\n",
        "print(\"Batch Answer Shape:\", batch[3].shape)  # Expected: [batch_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IHXQFM8pPm"
      },
      "source": [
        "**Step 8: Define VQA Model (CNN + LSTM)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z-wvuOjMfO8"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, answer_size, embed_dim=300, hidden_dim=512):\n",
        "        super(VQAModel, self).__init__()\n",
        "\n",
        "        # ðŸ”¹ IMAGE FEATURE EXTRACTOR (ResNet-50)\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove final classification layer\n",
        "        self.image_fc = nn.Linear(2048, hidden_dim)  # Reduce feature size\n",
        "\n",
        "        # ðŸ”¹ TEXT FEATURE EXTRACTOR (LSTM)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Word Embeddings\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # ðŸ”¹ FUSION & CLASSIFICATION\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 512)  # Merge Image + Text Features\n",
        "        self.fc2 = nn.Linear(512, answer_size)  # Final Classification Layer\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        # ðŸ”¹ IMAGE FEATURES\n",
        "        image_features = self.cnn(images)  # Output: [batch, 2048, 1, 1]\n",
        "        image_features = image_features.view(image_features.size(0), -1)  # Flatten: [batch, 2048]\n",
        "        image_features = self.image_fc(image_features)  # Reduce to [batch, hidden_dim]\n",
        "\n",
        "        # ðŸ”¹ TEXT FEATURES\n",
        "        # Ensure questions are in the correct format [batch_size, seq_len]\n",
        "        embedded = self.embedding(questions)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # If necessary, squeeze any extra dimension from the input\n",
        "        embedded = embedded.squeeze(1)  # This may be unnecessary if shape is already correct, but just in case\n",
        "\n",
        "        # Get the last hidden state from LSTM\n",
        "        _, (hidden, _) = self.lstm(embedded)  # Get last LSTM hidden state [batch, hidden_dim]\n",
        "        text_features = hidden[-1]  # Extract final hidden state\n",
        "\n",
        "        # ðŸ”¹ MERGE IMAGE + TEXT\n",
        "        combined = torch.cat((image_features, text_features), dim=1)  # [batch, hidden_dim * 2]\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        output = self.fc2(x)  # Output logits [batch, answer_size]\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8RyPz7w8vqi"
      },
      "source": [
        "**Step 9: Model, Criterion, and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0HyWgWgCSB0v",
        "outputId": "af7584cb-14a5-43eb-887b-dba25a79ebfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "vocab_size = 30522  # BERT tokenizer vocab size\n",
        "answer_size = len(answer_vocab)  # Number of unique answers\n",
        "model = VQAModel(vocab_size, answer_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TerpHJIhTOcN",
        "outputId": "3fd5a883-24cf-4f3b-dfe8-a8aeec27490f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torch_xla in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch torch_xla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z2Mw8JfiPHAk",
        "outputId": "d6adb726-eac4-4530-c082-1bbef52b9ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in multiple-choice questions: dict_keys(['info', 'task_type', 'data_type', 'license', 'questions', 'data_subtype', 'num_choices'])\n",
            "\n",
            "ðŸ”¹ Sample Multiple-Choice Question:\n",
            "{\n",
            "    \"image_id\": 11779,\n",
            "    \"question\": \"Who looks happier?\",\n",
            "    \"multiple_choices\": [\n",
            "        \"alive\",\n",
            "        \"1\",\n",
            "        \"woman\",\n",
            "        \"purple\",\n",
            "        \"2\",\n",
            "        \"yes\",\n",
            "        \"white\",\n",
            "        \"boy\",\n",
            "        \"she loves him\",\n",
            "        \"mountain\",\n",
            "        \"3\",\n",
            "        \"no\",\n",
            "        \"baby\",\n",
            "        \"man\",\n",
            "        \"yellow\",\n",
            "        \"red\",\n",
            "        \"4\",\n",
            "        \"blue\"\n",
            "    ],\n",
            "    \"question_id\": 117792\n",
            "}\n",
            "\n",
            "ðŸ”¹ Sample Answer:\n",
            "{\n",
            "    \"question_type\": \"who\",\n",
            "    \"multiple_choice_answer\": \"man\",\n",
            "    \"answers\": [\n",
            "        {\n",
            "            \"answer\": \"old person\",\n",
            "            \"answer_confidence\": \"maybe\",\n",
            "            \"answer_id\": 1\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"maybe\",\n",
            "            \"answer_id\": 2\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 3\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 4\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"old man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 5\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 6\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 7\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 8\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"man\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 9\n",
            "        },\n",
            "        {\n",
            "            \"answer\": \"grandpa\",\n",
            "            \"answer_confidence\": \"yes\",\n",
            "            \"answer_id\": 10\n",
            "        }\n",
            "    ],\n",
            "    \"image_id\": 11779,\n",
            "    \"answer_type\": \"other\",\n",
            "    \"question_id\": 117792\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the dataset\n",
        "with open(mcq_train_file, \"r\") as f:\n",
        "    mcq_train = json.load(f)\n",
        "\n",
        "with open(mcq_val_file, \"r\") as f:\n",
        "    mcq_val = json.load(f)\n",
        "\n",
        "with open(annotations_train_file, \"r\") as f:\n",
        "    annotations_train = json.load(f)\n",
        "\n",
        "with open(annotations_val_file, \"r\") as f:\n",
        "    annotations_val = json.load(f)\n",
        "\n",
        "# Print dataset structure\n",
        "print(\"Keys in multiple-choice questions:\", mcq_train.keys())\n",
        "print(\"\\nðŸ”¹ Sample Multiple-Choice Question:\")\n",
        "print(json.dumps(mcq_train[\"questions\"][0], indent=4))\n",
        "\n",
        "print(\"\\nðŸ”¹ Sample Answer:\")\n",
        "print(json.dumps(annotations_train[\"annotations\"][0], indent=4))\n",
        "\n",
        "# Image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load an image, apply transformations, and return a tensor.\"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(img)\n",
        "    return img_tensor\n",
        "\n",
        "# Text tokenization and encoding using BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_question(question):\n",
        "    \"\"\"Tokenizes and converts a question into a numerical tensor.\"\"\"\n",
        "    encoded_question = tokenizer(question, padding=\"max_length\", max_length=20, truncation=True, return_tensors=\"pt\")\n",
        "    return encoded_question[\"input_ids\"], encoded_question[\"attention_mask\"]\n",
        "\n",
        "# Answer encoding\n",
        "all_answers = [ann[\"multiple_choice_answer\"] for ann in annotations_train[\"annotations\"]]\n",
        "answer_vocab = {ans: idx for idx, ans in enumerate(set(all_answers))}\n",
        "\n",
        "def preprocess_answer(answer):\n",
        "    \"\"\"Converts an answer into a numerical label.\"\"\"\n",
        "    return answer_vocab.get(answer, -1)  # Return -1 if the answer is not found in vocab\n",
        "\n",
        "# Dataset creation\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, questions, annotations, image_dir):\n",
        "        self.questions = questions[\"questions\"]\n",
        "        self.annotations = annotations[\"annotations\"]\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question_text = self.questions[idx][\"question\"]\n",
        "        answer_text = self.annotations[idx][\"multiple_choice_answer\"]\n",
        "        image_id = self.questions[idx][\"image_id\"]\n",
        "        image_path = os.path.join(self.image_dir, f\"abstract_v002_train2015_{image_id:012d}.png\")\n",
        "\n",
        "        image_tensor = preprocess_image(image_path)\n",
        "        input_ids, attention_mask = preprocess_question(question_text)\n",
        "        answer_label = preprocess_answer(answer_text)\n",
        "\n",
        "        return image_tensor, input_ids, attention_mask, answer_label\n",
        "\n",
        "# Prepare DataLoader\n",
        "image_dir = os.path.join(dataset_dir, \"scene_img_abstract_v002_train2015\")\n",
        "vqa_dataset = VQADataset(mcq_train, annotations_train, image_dir)\n",
        "dataloader = DataLoader(vqa_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Define the VQA Model (CNN + LSTM)\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, answer_size, embed_dim=300, hidden_dim=512):\n",
        "        super(VQAModel, self).__init__()\n",
        "\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove final classification layer\n",
        "        self.image_fc = nn.Linear(2048, hidden_dim)  # Reduce feature size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Word Embeddings\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 512)  # Merge Image + Text Features\n",
        "        self.fc2 = nn.Linear(512, answer_size)  # Final Classification Layer\n",
        "\n",
        "    def forward(self, images, questions):\n",
        "        image_features = self.cnn(images)  # Output: [batch, 2048, 1, 1]\n",
        "        image_features = image_features.view(image_features.size(0), -1)  # Flatten: [batch, 2048]\n",
        "        image_features = self.image_fc(image_features)  # Reduce to [batch, hidden_dim]\n",
        "\n",
        "        embedded = self.embedding(questions)  # [batch, seq_len, embed_dim]\n",
        "        _, (hidden, _) = self.lstm(embedded)  # Get last LSTM hidden state [batch, hidden_dim]\n",
        "        text_features = hidden[-1]  # Extract final hidden state\n",
        "\n",
        "        combined = torch.cat((image_features, text_features), dim=1)  # [batch, hidden_dim * 2]\n",
        "        x = torch.relu(self.fc1(combined))\n",
        "        output = self.fc2(x)  # Output logits [batch, answer_size]\n",
        "\n",
        "        return output\n",
        "\n",
        "# Model, criterion, and optimizer\n",
        "vocab_size = 30522  # BERT tokenizer vocab size\n",
        "answer_size = len(answer_vocab)  # Number of unique answers\n",
        "model = VQAModel(vocab_size, answer_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in dataloader:\n",
        "        images, input_ids, labels = images.to(device), input_ids.squeeze(1).to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids)  # Forward pass\n",
        "\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Test the model\n",
        "def predict(image_path, question_text):\n",
        "    image_tensor = preprocess_image(image_path).unsqueeze(0).to(device)\n",
        "    input_ids, _ = preprocess_question(question_text)\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor, input_ids)\n",
        "        predicted_label = torch.argmax(F.softmax(output, dim=1), dim=1).item()\n",
        "\n",
        "    predicted_answer = [key for key, val in answer_vocab.items() if val == predicted_label][0]\n",
        "    return predicted_answer\n",
        "\n",
        "# Example\n",
        "test_image = \"/content/drive/MyDrive/vqa_abstract_dataset/scene_img_abstract_v002_train2015/abstract_v002_train2015_000000000144.png\"\n",
        "test_question = \"What is the man holding?\"\n",
        "\n",
        "predicted_answer = predict(test_image, test_question)\n",
        "print(\"Predicted Answer:\", predicted_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuW_pAL16mY_"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03z9J7656WdV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the model saving path\n",
        "checkpoint_path = \"/content/drive/MyDrive/vqa_abstract_dataset/vqa_model.pth\"\n",
        "\n",
        "# Inside the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, input_ids, attention_mask, labels in dataloader:\n",
        "        images, input_ids, labels = images.to(device), input_ids.squeeze(1).to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids)  # Forward pass\n",
        "\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "    # Save the model after every epoch\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "print(\"Model saved at:\", checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poMqeZoM6OuZ"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(checkpoint_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXloBUXw6ink"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    for batch_idx, (images, input_ids, attention_mask, labels) in enumerate(dataloader):\n",
        "        if batch_idx % 10 == 0:  # Print every 10th batch to monitor progress\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(dataloader)}\")\n",
        "\n",
        "        images, input_ids, labels = images.to(device), input_ids.squeeze(1).to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images, input_ids)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKftrn6kxpfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM2/iIPmUGO+9+Tk1KDp/xv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cb550a02504421b860db7b4a046edea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f42b7bb539924cbd9f3c4c5641a3a248",
              "IPY_MODEL_8b5c97fcef614be88baff2164720d0b9",
              "IPY_MODEL_5f8e9872452446a0ab4870f2edb5c543"
            ],
            "layout": "IPY_MODEL_c6a5c81450444f7ca98d83ab162156d8"
          }
        },
        "f42b7bb539924cbd9f3c4c5641a3a248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801a41e7449c4604b96946dffee9d3ca",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7a32c777a414dbe94b3eb65a4209894",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "8b5c97fcef614be88baff2164720d0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5496531b69504e348936569cfbfc4c52",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e572cdfcccc4229836add10e8544e31",
            "value": 48
          }
        },
        "5f8e9872452446a0ab4870f2edb5c543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e796e9652b2e4b3cab3d8dece7e22bee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_de3831d00cfc40eb910b35bed4daf86d",
            "value": "â€‡48.0/48.0â€‡[00:00&lt;00:00,â€‡872B/s]"
          }
        },
        "c6a5c81450444f7ca98d83ab162156d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "801a41e7449c4604b96946dffee9d3ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a32c777a414dbe94b3eb65a4209894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5496531b69504e348936569cfbfc4c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e572cdfcccc4229836add10e8544e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e796e9652b2e4b3cab3d8dece7e22bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3831d00cfc40eb910b35bed4daf86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3da1cb626a24418a4c94583709b1304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a153faae67e4df2b2db22ef77dd0557",
              "IPY_MODEL_d3a8f94c0ed746e49eac52d825985205",
              "IPY_MODEL_4dcac941ba314319b9ab320ddcbcbf35"
            ],
            "layout": "IPY_MODEL_e3e1ff04016144308f8ce4e973c00b77"
          }
        },
        "6a153faae67e4df2b2db22ef77dd0557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a126fd834f2942bfb7939220ba71d55b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2af0a2cc51984cbe938fcd6d74be05a2",
            "value": "vocab.txt:â€‡100%"
          }
        },
        "d3a8f94c0ed746e49eac52d825985205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ccfcbb680d04dd39c166b756d9192e4",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74c39715aaaf4d85b510e4a285c810fb",
            "value": 231508
          }
        },
        "4dcac941ba314319b9ab320ddcbcbf35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4412530bcac945738847453724c1cf8f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ebfb2e30104e4f4b931cc75f5564f51e",
            "value": "â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡3.43MB/s]"
          }
        },
        "e3e1ff04016144308f8ce4e973c00b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a126fd834f2942bfb7939220ba71d55b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af0a2cc51984cbe938fcd6d74be05a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ccfcbb680d04dd39c166b756d9192e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74c39715aaaf4d85b510e4a285c810fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4412530bcac945738847453724c1cf8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebfb2e30104e4f4b931cc75f5564f51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab0de5bcf3c048538e79957abc93765c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d39c33840c94952b6a40ed8c93dbb9e",
              "IPY_MODEL_ef6a51e1ae244b4aad333f3f3be6a640",
              "IPY_MODEL_970366c634db4ded990ce5c700dbc6fe"
            ],
            "layout": "IPY_MODEL_fdca40e819ef4c22b9901878e6a2959c"
          }
        },
        "8d39c33840c94952b6a40ed8c93dbb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9a110bfb4e546b9afd587794965461f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5ef1fad53c134cbd9b057890ef060500",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "ef6a51e1ae244b4aad333f3f3be6a640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd6695a935548709c0ed6ab10344e6e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bb9cc6c4c094ce18d0ebc26ff596bed",
            "value": 466062
          }
        },
        "970366c634db4ded990ce5c700dbc6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae82c4407ed4e45a254c51d58c5b6d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e92565ce26814c2d903bbe51f42c4faf",
            "value": "â€‡466k/466kâ€‡[00:00&lt;00:00,â€‡8.86MB/s]"
          }
        },
        "fdca40e819ef4c22b9901878e6a2959c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a110bfb4e546b9afd587794965461f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ef1fad53c134cbd9b057890ef060500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd6695a935548709c0ed6ab10344e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bb9cc6c4c094ce18d0ebc26ff596bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dae82c4407ed4e45a254c51d58c5b6d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92565ce26814c2d903bbe51f42c4faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f807ddaa4959446c954e84bc91aff463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175bcdb734664fe19546bf68090f2606",
              "IPY_MODEL_456c2303139f4654a32cb3d85cd377f8",
              "IPY_MODEL_915a9890db86458792c57166bfb7be4f"
            ],
            "layout": "IPY_MODEL_08a257b8c4bf41ee90b14a1eb325dd21"
          }
        },
        "175bcdb734664fe19546bf68090f2606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec25e27bc3047dab11941948424558e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a1d07bb852549509beebdbffa12cb40",
            "value": "config.json:â€‡100%"
          }
        },
        "456c2303139f4654a32cb3d85cd377f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1ed8c70bd60488688a214df0b86cb62",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5289ac724324107ad10953adb44d572",
            "value": 570
          }
        },
        "915a9890db86458792c57166bfb7be4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15b0e36810944907b932ddf1cfeb14e5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_87d3da753db745b686205112d97dd63d",
            "value": "â€‡570/570â€‡[00:00&lt;00:00,â€‡16.6kB/s]"
          }
        },
        "08a257b8c4bf41ee90b14a1eb325dd21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec25e27bc3047dab11941948424558e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a1d07bb852549509beebdbffa12cb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1ed8c70bd60488688a214df0b86cb62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5289ac724324107ad10953adb44d572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15b0e36810944907b932ddf1cfeb14e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d3da753db745b686205112d97dd63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}